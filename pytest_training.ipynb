{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit testing in Python with pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "author: **Filip Noworolnik**  \n",
    "email: fnoworolnik@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **unit** - smallest testable part\n",
    "* testing individual units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example system:    \n",
    "> cook <- waiter <- customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Example reference](https://stackoverflow.com/questions/3622455/what-is-the-purpose-of-mock-objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple test example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "def add(a, b):\n",
    "    return a+b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "from pytest_training import functions\n",
    "\n",
    "def test_add():\n",
    "    assert functions.add(1, 2) == 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run:\n",
    "    `\n",
    "    > pytest\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keyword: **assert**\n",
    "- auto-discovering tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest-coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it when you want to know if your tests cover all of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def t_or_f(x):\n",
    "    if x < 5:\n",
    "        return True\n",
    "    elif x < 10:\n",
    "        return False\n",
    "    else:\n",
    "        return None\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions2.py\n",
    "def multiply(a, b):\n",
    "    return a*b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>To run: `> pytest --cov:proj_name tests/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see in terminal which lines are not covered use `--cov-report term-missing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to skip fully covered modules in report. Just use: `--cov-report term:skip-covered`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use both together:  `> pytest --cov-report term-missing:skip-covered --cov=proj_name tests/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reports can be included in html, xml or annotated source code files instead of terminal:\n",
    "```shell\n",
    "pytest --cov-report html:cov_html\n",
    "        --cov-report xml:cov.xml\n",
    "        --cov-report annotate:cov_annotate\n",
    "        --cov=proj_name tests/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Tip:** Don't omit your tests in coverage!  \n",
    "If you have some helper code, not used anymore or odd part that you forget why is there - coverage would inform you about it.  \n",
    "    It would also alert you if your tests have identical names (*Tests require names, but the names don't matter, because nothing calls the names directly. If there are two tests with identical names - you have only one test, because the second will overwrite the first one!*).  \n",
    "    *Flake8*, *pylint* and some other linting tools can also alert you on some mistakes like same-named tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Include multiple tests in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "class TestTof:\n",
    "    def test_tof_1(self):\n",
    "        assert functions.t_or_f(4) is True\n",
    "\n",
    "    def test_tof_2(self):\n",
    "        assert functions.t_or_f(7) is False\n",
    "\n",
    "    def test_tof_3(self):\n",
    "        assert functions.t_or_f(15) is None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use multiple asserts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def test_tof():\n",
    "    assert functions.t_or_f(4) is True\n",
    "    assert functions.t_or_f(7) is False\n",
    "    assert functions.t_or_f(15) is None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parametrize test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.parametrize('test_input, expected', [(4, True), (7, False), (15, None)])\n",
    "def test_tof_param(test_input, expected):\n",
    "    assert functions.t_or_f(test_input) is expected\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixtures are objects that can be requested by the test functions or other fixtures by declaring them as argument names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@pytest.fixture\n",
    "def db_session(tmpdir):\n",
    "    fn = tmpdir / \"db.file\"\n",
    "    return connect(str(fn))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tmp_path` fixture provides a temporary directory unique to test invocation created in base temporary directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "CONTENT = \"content\"\n",
    "\n",
    "def test_create_file(tmp_path):\n",
    "    d = tmp_path / \"sub\"\n",
    "    d.mkdir()\n",
    "    p = d / \"hello.txt\"\n",
    "    p.write_text(CONTENT)\n",
    "    assert p.read_text() == CONTENT\n",
    "    assert len(list(tmp_path.iterdir())) == 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skipping tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want skip test (e.g. part of code is not yet implemented, so you don't want to test it), you can mark test to be skipped using `@skip` decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def not_implemented(a, b):\n",
    "    ...\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.skip(reason='Function is not implemented yet...')\n",
    "def test_not_implemented():\n",
    "    assert functions.not_implemented(5, 10) is True\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reason` argument contains info about why the test is skipped and is included in report.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  `skip()` can be also called as a function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def test_not_implemented2():\n",
    "    pytest.skip('Function not implemented.')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes test have to be skipped under certain conditions, e.g. when test run on windows and function works only on linux.  \n",
    "To achieve this, another function/decorator would be usefull: `skipif`\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def linux_function():\n",
    "    print(\"This is linux-only function\")\n",
    "    if not sys.platform.startswith(\"win\"):\n",
    "        return True\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"Test run on windows\")\n",
    "def test_linux_only():\n",
    "    assert functions.linux_function() is True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xfail, xpassed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you expect the test to fail. You can mark it with `xfail`. The test will be run but no traceback will be reported when it fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def impossible_function():\n",
    "    print(\"This method runs only with certain configuration\")\n",
    "    response = requests.get('fifinow.com/amazing')\n",
    "    return response.text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_impossible():\n",
    "    assert functions.impossible_function() == 'Filip is amazing!'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run: `> pytest -rxXs`\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If test unexpectedly pass, the report will mark it as `xpass`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def response_mock():\n",
    "    return 'Filip is amazing!'\n",
    "\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_impossible2(monkeypatch):\n",
    "    monkeypatch.setattr(functions, 'impossible_function', response_mock)\n",
    "    assert functions.impossible_function() == 'Filip is amazing!'\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be more specific as to why the test is failing, you can specify exceptions in the `raises` argument.  \n",
    "The test will be reported as a regular failure if it fails with an exception not mentioned in `raises`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def raises():\n",
    "    x = list()\n",
    "    return x[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.xfail(raises=IndexError)\n",
    "def test_raises():\n",
    "    assert functions.raises() == 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking, monkey patching, and faking functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test waiter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> cook <- waiter <- test driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. test driver orders dishes and ensure the waiter returns correct dish\n",
    "2. test depends on the cook:\n",
    "    * cook may have non-deterministic behaviour (menu includes chef's surprise)\n",
    "    * may have a lot of dependencies (won't cook without his entire staff)\n",
    "    * may need a lot of resources (expensive ingredients, may take an hour to cook)\n",
    "3. cook 'in cahoots' with test driver:\n",
    "> test cook <- waiter <- test driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cook can be implemented in different ways:\n",
    "* fake cook - predending to be cook - using frozen dishes and microwave\n",
    "* stub cook - hot dog vendor - always gives you hot dogs, no matter what order is\n",
    "* mock cook - undercover cop, working with test driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Monkey patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monkey patching allows you to intercept what a function would normally do. You can specify your own return value ans substitute it's full execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def response_mock():\n",
    "    return 'Filip is amazing!'\n",
    "\n",
    "@pytest.fixture\n",
    "def fix_response(monkeypatch):\n",
    "    import functions\n",
    "    monkeypatch.setattr(functions, 'impossible_function', response_mock)\n",
    "\n",
    "def test_impossible_mpatched(fix_response):\n",
    "    assert functions.impossible_function() == 'Filip is amazing!'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we created the substitute function using a `monkeypatch` object. Treating the `functions` module as an object, `monkeypatch` changes the behaviour of `impossible_function` inside the module when called to this test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to replace `impossible_function` behaviour across every test, you can use `autouse` keyword argument in `pytest.fixture`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def response_mock():\n",
    "    return 'Filip is amazing!'\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def fix_response(monkeypatch):\n",
    "    import functions\n",
    "    monkeypatch.setattr(functions, 'impossible_function', response_mock)\n",
    "\n",
    "def test_impossible_mpatched():\n",
    "    assert functions.impossible_function() == 'Filip is amazing!'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite of comfort of not having to include fixture as a parameter for every test, I would not recommend using it because of the risk of not being able to test the function after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MagicMock - faking functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we need to patch over more than single function, like an instance of a whole object. This is a lot of work that we want to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def getting_response(request):\n",
    "    if request.method == 'GET':\n",
    "        return {}\n",
    "    if request.method == 'POST':\n",
    "        return request.POST\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def test_getting_response_get(mocker):\n",
    "    from pytest_training.functions import getting_response\n",
    "    req = mocker.MagicMock()\n",
    "    req.method = 'GET'\n",
    "    assert getting_response(req) == {}\n",
    "    \n",
    "    \n",
    "def test_getting_response_post(mocker):\n",
    "    from pytest_training.functions import getting_response\n",
    "    req = mocker.MagicMock()\n",
    "    req.method = 'POST'\n",
    "    req.POST = {'title': 'title', 'body': 'body'}\n",
    "    assert getting_response(req) == {'title': 'title', 'body': 'body'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created `MagicMock` object included in `pytest-mock` package with `method` attribute that has a value of `'GET'` in the first test and `'POST'` in the second. `POST` attribute is a dictionary containing some values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MagicMock` takes whatever form and functionality we need for the moment. It's great tool for unit testing that don't require checking for side effects. We may also mock side effects by providing `side_effects` keyword argument but when testing functions interconnecting with other parts of code we may want to choose a different testing method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't need to completely hijack function, but keep track of it's usage, `mocker.spy` object from `pytes-mock` package is really usefull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "class Numbers(object):\n",
    "    def __init__(self, iterable):\n",
    "        self._container = iterable\n",
    "\n",
    "    def make_unique(self):\n",
    "        i = 0\n",
    "        visited = []\n",
    "        while i < len(self._container):\n",
    "            if self._container[i] in visited:\n",
    "                self._drop_val(i)\n",
    "                i = 0\n",
    "                continue\n",
    "            visited.append(self._container[i])\n",
    "            i += 1\n",
    "\n",
    "    def _drop_val(self, idx):\n",
    "        self._container.pop(idx)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def test_values_are_dropped_if_already_seen(mocker):\n",
    "    nums = Numbers([1,2,1,2,1,2])\n",
    "    mocker.spy(nums, '_drop_val')\n",
    "    nums.make_unique()\n",
    "    assert nums._make_unique.call_count == 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMNI example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pytest\n",
    "\n",
    "project_suids = {'TEE': 'HSBCKSHCXJSHSBFHFAS', 'CCU': 'DUFNCSHCXJSHSBIRUTJ'}\n",
    "\n",
    "\n",
    "def return_suid():\n",
    "    return project_suids\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def setup_creta(mocker, monkeypatch):\n",
    "    mock_creta = mocker.MagicMock()\n",
    "    mock_creta.get_project_suid.side_effect = return_suid\n",
    "    return mock_creta\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def setup_msg_handler(mocker):\n",
    "    msg_handler = mocker.MagicMock()\n",
    "    return msg_handler\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def setup_logger(mocker):\n",
    "    logger = mocker.MagicMock()\n",
    "    return logger\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def setup_omni(setup_creta, setup_logger, tmpdir, setup_msg_handler):\n",
    "    from omni.omni import Omni\n",
    "    omni = Omni(setup_creta, setup_logger, tmpdir, setup_msg_handler)\n",
    "    return omni\n",
    "\n",
    "\n",
    "def test_run(setup_omni):\n",
    "    assert setup_omni.run_omni() is True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
