{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit testing in Python with pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple test example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "def add(a, b):\n",
    "    return a+b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "from pytest_training import functions\n",
    "\n",
    "def test_add():\n",
    "    assert functions.add(1, 2) == 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run:\n",
    "    `\n",
    "    > pytest\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keyword: **assert**\n",
    "- auto-discovering tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest-coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it when you want to know if your tests cover all of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def t_or_f(x):\n",
    "    if x < 5:\n",
    "        return True\n",
    "    elif x < 10:\n",
    "        return False\n",
    "    else:\n",
    "        return None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions2.py\n",
    "def multiply(a, b):\n",
    "    return a*b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>To run: `> pytest --cov:proj_name tests/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see in terminal which lines are not covered use `--cov-report term-missing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to skip fully covered modules in report. Just use: `--cov-report term:skip-covered`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use both together:  `> pytest --cov-report term-missing:skip-covered --cov=proj_name tests/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reports can be included in html, xml or annotated source code files instead of terminal:\n",
    "```\n",
    "pytest --cov-report html:cov_html\n",
    "        --cov-report xml:cov.xml\n",
    "        --cov-report annotate:cov_annotate\n",
    "        --cov=proj_name tests/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Include multiple tests in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "class TestTof:\n",
    "    def test_tof_1(self):\n",
    "        assert functions.t_or_f(4) is True\n",
    "\n",
    "    def test_tof_2(self):\n",
    "        assert functions.t_or_f(7) is False\n",
    "\n",
    "    def test_tof_3(self):\n",
    "        assert functions.t_or_f(15) is None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use multiple asserts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def test_tof():\n",
    "    assert functions.t_or_f(4) is True\n",
    "    assert functions.t_or_f(7) is False\n",
    "    assert functions.t_or_f(15) is None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parametrize test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.parametrize('test_input, expected', [(4, True), (7, False), (15, None)])\n",
    "def test_tof_param(test_input, expected):\n",
    "    assert functions.t_or_f(test_input) is expected\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skipping tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want skip test (e.g. part of code is not yet implemented, so you don't want to test it), you can mark test to be skipped using `@skip` decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def not_implemented(a, b):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.skip(reason='Function is not implemented yet...')\n",
    "def test_not_implemented():\n",
    "    assert functions.not_implemented(5, 10) is True\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reason` argument contains info about why the test is skipped and is included in report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  `skip()` can be also called as a function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def test_not_implemented2():\n",
    "    pytest.skip('Function not implemented.')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes test have to be skipped under certain conditions, e.g. when test run on windows and function works only on linux.  \n",
    "To achieve this, another function/decorator would be usefull: `skipif`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def linux_function():\n",
    "    print(\"This is linux-only function\")\n",
    "    if not sys.platform.startswith(\"win\"):\n",
    "        return True\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"Test run on windows\")\n",
    "def test_linux_only():\n",
    "    assert functions.linux_function() is True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xfail, xpassed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you expect the test to fail. You can mark it with `xfail`. The test will be run but no traceback will be reported when it fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def impossible_function():\n",
    "    print(\"This method runs only with certain configuration\")\n",
    "    response = requests.get('fifinow.com/amazing')\n",
    "    return response.text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_impossible():\n",
    "    assert functions.impossible_function() == 'Filip is amazing!'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run: `> pytest -rxXs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If test unexpectedly pass, the report will mark it as `xpass`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def response_mock():\n",
    "    return 'Filip is amazing!'\n",
    "\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_impossible2(monkeypatch):\n",
    "    monkeypatch.setattr(functions, 'impossible_function', response_mock)\n",
    "    assert functions.impossible_function() == 'Filip is amazing!'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be more specific as to why the test is failing, you can specify exceptions in the `raises` argument.  \n",
    "The test will be reported as a regular failure if it fails with an exception not mentioned in `raises`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# functions.py\n",
    "(...)\n",
    "\n",
    "def raises():\n",
    "    x = list()\n",
    "    return x[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "@pytest.mark.xfail(raises=IndexError)\n",
    "def test_raises():\n",
    "    assert functions.raises() == 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking, monkey patching, and faking functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monkey patching allows you to intercept what a function would normally do. You can specify your own return value ans substitute it's full execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def response_mock():\n",
    "    return 'Filip is amazing!'\n",
    "\n",
    "@pytest.fixture\n",
    "def fix_response(monkeypatch):\n",
    "    import functions\n",
    "    monkeypatch.setattr(functions, 'impossible_function', response_mock)\n",
    "\n",
    "def test_impossible_mpatched(fix_response):\n",
    "    assert functions.impossible_function() == 'Filip is amazing!'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we created the substitute function using a `monkeypatch` object included in `pytest-mock` package. Treating the `functions` module as an object, monkeypatch changes the behaviour of `impossible_function` inside the module when called to this test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to replace `impossible_function` behaviour across every test, you can use `autouse` keyword argument in `pytest.fixture`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_functions.py\n",
    "(...)\n",
    "\n",
    "def response_mock():\n",
    "    return 'Filip is amazing!'\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def fix_response(monkeypatch):\n",
    "    import functions\n",
    "    monkeypatch.setattr(functions, 'impossible_function', response_mock)\n",
    "\n",
    "def test_impossible_mpatched():\n",
    "    assert functions.impossible_function() == 'Filip is amazing!'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite of comfort of not having to include fixture as a parameter for every test, I would not recommend using it because of the risk of not being able to test the function after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
